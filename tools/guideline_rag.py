#ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ë„êµ¬
from typing import Dict, List, Any, Optional
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, TextLoader
import os
import json

class GuidelineRAG:
    """
    AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ì •ë³´ ê²€ìƒ‰ ë„êµ¬(ë¡œì»¬ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
    """

    def __init__(self, model_name="gpt-4o-mini", embeddings_model="text-embedding-3-small"):
        # ì„ë² ë”© ë° LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(model=embeddings_model)
        self.llm = ChatOpenAI(model=model_name, temperature=0.2)
        
        # ë²¡í„° ì €ì¥ì†Œ ë° ê°€ì´ë“œë¼ì¸ ì •ë³´ ì´ˆê¸°í™”
        self.vector_store = None
        self.guidelines_info = {}
        
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ë¡œì»¬ PDF/TXT íŒŒì¼ ë¡œë“œ)
        self._initialize_vector_store()
        
        # API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        
    def _initialize_vector_store(self):
        """ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        guidelines_dir = "data/guidelines"
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(guidelines_dir):
            os.makedirs(guidelines_dir, exist_ok=True)
            # ê¸°ë³¸ ê°€ì´ë“œë¼ì¸ íŒŒì¼ ìƒì„±
            self._create_sample_guidelines(guidelines_dir) #ìƒ˜í”Œ íŒŒì¼ ìƒì„±
        
        # ë¬¸ì„œ ë¡œë“œ
        documents = []
        
        for filename in os.listdir(guidelines_dir):
            filepath = os.path.join(guidelines_dir, filename)
            
            try:
                #PDF íŒŒì¼ ì²˜ë¦¬
                if filename.endswith('.pdf'):
                    loader = PyPDFLoader(filepath)
                    file_docs = loader.load()
                    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— ì¶œì²˜ ì¶”ê°€
                    for doc in file_docs:
                        doc.metadata["source"] = filename
                    documents.extend(file_docs)
                    self.guidelines_info[filename] = {"type": "pdf", "pages": len(file_docs)}
                #í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
                elif filename.endswith('.txt'):
                    loader = TextLoader(filepath)
                    file_docs = loader.load()
                    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— ì¶œì²˜ ì¶”ê°€
                    for doc in file_docs:
                        doc.metadata["source"] = filename
                    documents.extend(file_docs)
                    self.guidelines_info[filename] = {"type": "text", "pages": len(file_docs)}
            
            except Exception as e:
                print(f"âš ï¸ {filename} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        #ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not documents:
            print("âš ï¸ ê°€ì´ë“œë¼ì¸ ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(documents)
        
        # ë²¡í„° ì €ì¥ì†Œ(ìŠ¤í† ì–´) ìƒì„± -> ë””ìŠ¤í¬ ì €ì¥ X ë©”ëª¨ë¦¬ ì €ì¥ ì½”ë“œ
        self.vector_store = FAISS.from_documents(splits, self.embeddings)
        print(f"âœ… {len(splits)}ê°œì˜ ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ì²­í¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def search_web(self, query: str) -> str:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if not self.serpapi_key:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë©”ì‹œì§€ ë°˜í™˜
            return "ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ SERPAPI_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        
        try:
            # SerpAPIë¥¼ ì´ìš©í•œ ì›¹ ê²€ìƒ‰
            url = "https://serpapi.com/search"
            params = {
                "q": query,
                "api_key": self.serpapi_key,
                "num": 3  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ
            }
            
            response = requests.get(url, params=params)
            data = response.json()
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
            results = []
            if "organic_results" in data:
                for result in data["organic_results"][:3]:
                    results.append(f"ì œëª©: {result.get('title')}\n"
                                 f"ìš”ì•½: {result.get('snippet')}\n"
                                 f"ë§í¬: {result.get('link')}\n")
            
            return "\n\n".join(results)
            
        except Exception as e:
            return f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

    def query_hybrid(self, query: str, domain_info: str = "", 
                    ethical_aspect: str = "", use_web_search: bool = True) -> Dict[str, Any]:
        """ë¡œì»¬ ê°€ì´ë“œë¼ì¸ê³¼ ì›¹ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ê°œì„  (ë„ë©”ì¸ê³¼ ìœ¤ë¦¬ì  ì¸¡ë©´ í¬í•¨)
        enhanced_query = f"{query} {domain_info} {ethical_aspect} AI ìœ¤ë¦¬"
        
        # 1. ë¡œì»¬ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
        local_results = self.search_local_guidelines(enhanced_query)
        
        # 2. ì›¹ ê²€ìƒ‰ (ì„ íƒì )
        web_results = ""
        if use_web_search:
            print("ğŸ” ì›¹ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰ ì¤‘...")
            web_results = self.search_web(enhanced_query)
        
        # 3. ê²°ê³¼ ê²°í•© ë° ë¶„ì„
        combined_info = self._combine_and_analyze(query, local_results, web_results, domain_info, ethical_aspect)
        
        return {
            "local_results": local_results,
            "web_results": web_results if use_web_search else "ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì•ˆí•¨",
            "combined_analysis": combined_info
        }
    
    def search_local_guidelines(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """ë¡œì»¬ ê°€ì´ë“œë¼ì¸ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
        # ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if not self.vector_store:
            return []
        
        # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰
        results = self.vector_store.similarity_search_with_score(query, k=n_results)
        
        # ê²°ê³¼ ê°€ê³µ
        formatted_results = []
        for doc, score in results:
            # ì ìˆ˜ë¥¼ ê´€ë ¨ë„ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)
            relevance = 1.0 / (1.0 + score)
            
            formatted_results.append({
                "content": doc.page_content,
                "source": doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "relevance": round(relevance, 3)
            })
        
        return formatted_results
    
    def _combine_and_analyze(self, query: str, local_results: List[Dict[str, Any]], 
                           web_results: str, domain_info: str, ethical_aspect: str) -> str:
        """ë¡œì»¬ ê°€ì´ë“œë¼ì¸ê³¼ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ë¶„ì„"""
        # ë¡œì»¬ ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        local_texts = []
        for result in local_results:
            local_texts.append(f"ì¶œì²˜: {result['source']} (ê´€ë ¨ë„: {result['relevance']})\n"
                             f"ë‚´ìš©: {result['content']}")
        
        local_content = "\n\n".join(local_texts)
        
        # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
        ë‹¤ìŒì€ "{query}"ì— ê´€í•œ ì •ë³´ì…ë‹ˆë‹¤:
        
        ## ë¡œì»¬ ê°€ì´ë“œë¼ì¸ ì •ë³´:
        {local_content}
        
        ## ì›¹ ê²€ìƒ‰ ê²°ê³¼:
        {web_results}
        
        ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ {domain_info} ë„ë©”ì¸ì—ì„œ {ethical_aspect} ê´€ë ¨ AI ìœ¤ë¦¬ ì§€ì¹¨ê³¼ 
        ëª¨ë²” ì‚¬ë¡€ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. ë¡œì»¬ ê°€ì´ë“œë¼ì¸ì˜ í•µì‹¬ ì›ì¹™ê³¼ ì›¹ì—ì„œ ì°¾ì€ ìµœì‹  ì •ë³´ë¥¼ 
        ê²°í•©í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        """
        
        # LLMìœ¼ë¡œ ì¢…í•© ë¶„ì„
        response = self.llm.invoke(prompt)
        return response.content

    # ê¸°ì¡´ ë©”ì†Œë“œë“¤ì€ ê°„ë‹¨í•˜ê²Œ ìœ ì§€ (í˜¸í™˜ì„± ìœ„í•´)
    def query_guidelines(self, query: str, domain_info: Optional[str] = None, 
                      ethical_aspect: Optional[str] = None) -> Dict[str, Any]:
        """ê¸°ì¡´ APIì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì†Œë“œ"""
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í˜¸ì¶œ
        results = self.query_hybrid(
            query=query,
            domain_info=domain_info or "",
            ethical_aspect=ethical_aspect or ""
        )
        
        return {
            "search_results": results["local_results"],
            "web_results": results["web_results"],
            "combined_analysis": results["combined_analysis"],
            "recommended_guidelines": self._get_recommended_guidelines(results["local_results"])
        }
    
    def _get_recommended_guidelines(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¶”ì²œ ê°€ì´ë“œë¼ì¸ ì¶”ì¶œ"""
        recommended = []
        for result in results:
            source = result["source"]
            if source not in [g.get("source") for g in recommended]:
                recommended.append({
                    "source": source,
                    "relevance": result["relevance"]
                })
        
        return sorted(recommended, key=lambda x: x["relevance"], reverse=True)

    def _create_sample_guidelines(self, directory: str):
        """ìƒ˜í”Œ ê°€ì´ë“œë¼ì¸ íŒŒì¼ ìƒì„± (í•„ìš”ì‹œ)"""
        # ìƒ˜í”Œ ë°ì´í„° ìƒëµ - ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€
        pass
